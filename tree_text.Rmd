---
title: "Data analytics for economists: Assignment I"
author: "Robin"
date: "2/27/2021"
output: 
  html_document:
    number_sections: TRUE
---

<style type="text/css">
body{ font-size: 15pt;}
pre {font-size: 12px;}
p {line-height: 1.5em;}
p {margin-bottom: 1em;}
p {margin-top: 1em;}
</style>

```{r include=FALSE}
knitr::opts_chunk$set(cache=F) # change it to T if you want to store results already produced. Note that the saved results will not be overwritten if you did not name each code chunk. Use it with caution.
```




***


# Document overhead and initialisation

##Read in packages

```{r}
library(data.table)
library(ggplot2)


library(tuneRanger)
library(mlr)
library(OpenML)

library(e1071)

pacman::p_load(rpart, rpart.plot, vip, pROC, randomForest, gbm)


```

## Download the data and present some basic statistics.
1 Download test and training sets. 
-- Training = DT
-- Test = test_submission

2 create copies of the data to enable processing of different data sets for different methods

```{r}
DT_raw <- fread("https://www.dropbox.com/s/5rr2ysw6tjcnbpj/train.csv?dl=1")
summary(DT_raw)

test_submission_raw <- fread("https://www.dropbox.com/s/395thqjfxf7k4wi/test.csv?dl=1")

DT_fact <- copy(DT_raw)
test_submission_fact <- copy(test_submission_raw)


```

Something we do note from the distribution of wage shows some outliers. We hypothesise this is the top players such as the Ronaldo's and Messi's of the world. We however also must not that there is likely no (linear) relationship to only the skill of the player. Likely these players their salary is in part so high since they are very popular with fans and bring in additional revenue through that popularity (e.g. sponsorship deals.).

The histogram below depicts the effect very well that was hinted at by the summary statistics

```{r}
hist(DT_raw$wage_eur,  breaks = 1500, xlab = "Wage in Euros")
```


## Some data processing is needed to meld the data into certain applications 
Factorize data. Manual process to ensure consistency for both the test and training sets

```{r}
#DT dataset
DT_fact$preferred_foot <- invisible(sapply(DT_fact$preferred_foot,switch,'Right'=1,'Left'=2,'undefined'=""))
DT_fact$work_rate <- invisible(sapply(DT_fact$work_rate,switch,'Low/Medium'=1,'High/Medium'=2,'Medium/Medium'=3,'Medium/Low'=4,'Medium/High'=5,'High/High'=6,'High/Low'=7,'Low/Low'=8,'Low/High'=9,'undefined'=""))
DT_fact$team_position <- invisible(sapply(DT_fact$team_position,switch,'LCB'=1,'SUB'=2,'RES'=3,'LS'=4,'RB'=5,'ST'=6,'RM'=7,'LCM'=8,'LB'=9,'LM'=10,'RCB'=11,'RW'=12,'CAM'=13,'RDM'=14,'RWB'=15,'LF'=16,'CDM'=17,'LW'=18,'CB'=19,'LDM'=20,'CM'=21,'RCM'=22,'LAM'=23,'CF'=24,'RS'=25,'RF'=26,'RAM'=27,'LWB'=28,'undefined'=""))

#Test submission factorization. Identical factorisation criteria
test_submission_fact$preferred_foot <- invisible(sapply(test_submission_fact$preferred_foot,switch,'Right'=1,'Left'=2,'undefined'=""))
test_submission_fact$work_rate <- invisible(sapply(test_submission_fact$work_rate,switch,'Low/Medium'=1,'High/Medium'=2,'Medium/Medium'=3,'Medium/Low'=4,'Medium/High'=5,'High/High'=6,'High/Low'=7,'Low/Low'=8,'Low/High'=9,'undefined'=""))
test_submission_fact$team_position <- invisible(sapply(test_submission_fact$team_position,switch,'LCB'=1,'SUB'=2,'RES'=3,'LS'=4,'RB'=5,'ST'=6,'RM'=7,'LCM'=8,'LB'=9,'LM'=10,'RCB'=11,'RW'=12,'CAM'=13,'RDM'=14,'RWB'=15,'LF'=16,'CDM'=17,'LW'=18,'CB'=19,'LDM'=20,'CM'=21,'RCM'=22,'LAM'=23,'CF'=24,'RS'=25,'RF'=26,'RAM'=27,'LWB'=28,'undefined'=""))

```


### Important parameters defining the regression model
```{r}
Data_splitting_seed_set = seq(120,124, by = 1) #seeds through which ten folds of the data are created for evaluation of methods
Model_stochastics_seed_set = seq(200,204, by = 1) #set of seeds to initiate stochastic models. aids in reproducibility

```


# Regression methods

In the section below we discuss and evaluate several machine leaning methods to predict footballers wage as a function of a broad set of performance criteria. The section and methods can be broadly broken up into 4 main methods:

1 regression-based methods
2 tree-based methods
3 support vector machine 
4 neural networks & ensemble methods

Each section will enjoy discussion and will feature several sub-methods. The method for comparison is mainly focused around the Mean Squared Error or MSE for short. This metric is easy to compute and facilitates easy comparison and aggreation between methods.Of course, as the sections will show, other metrics are considered in the tuning and selection of methods. Finally, the best performing model will be highlighted in a seperate subsection alongside which a discussion of the great performance of this method will be given.


## regression-based methods

### OLS
We start with the most basic method evaluated in the course of this document Ordinary Least Squared or OLS for short. OLS determines the best linear combination of estimators to determine the independent variable. In order to get a robust statistic for OLS (and as we will repeat for every method) we generate several folds of data according to the  seed parameters defined earlier in the document.



```{r}

OLS_outcome <- matrix(NA, nrow=10, ncol=2)
colnames(OLS_outcome) <- c("data_split_seed","MSE")
i = 1

for (data_seed_value in Data_splitting_seed_set){
  #create 80/20 split based on the data split seed. (outer loop)
  set.seed(data_seed_value)
  train_ind <- sample(1:nrow(DT_fact), 0.8*nrow(DT_fact))
  
  train <- DT_fact[train_ind,]
  test <- DT_fact[-train_ind,]
  
  Y_test <- test$wage_eur
  
  #teach actual model
  OLS_mod <- lm(wage_eur ~ ., train[, -c("player_ID")])
  
  #after model is made, calculate the MSE
  Y_pred_ols <- predict(OLS_mod, test)
  MSE <- mean((Y_test - Y_pred_ols)^2)
  
  #save results
  OLS_outcome[i, "data_split_seed"] <-  data_seed_value
  OLS_outcome[i, "MSE"] <-  MSE
  
  #next row for next result
  i = i+1
}

```


### A more advanced method considered was the elasticnet. Elasticnet is a combination of Ridge and Lasso regression.

HUONG will do this section!





# Tree based models
Tree based models are another popular modeling method in Machine learning. ALthough often though of as a classification method, Trees can also be used to make numerical predictions. In the section below a handful of tree based methods are evaluated. Overall the tree based metrics are some of the best performing of all evaluated methods. 

Tree based methods are oftentimes stochastic in nature. In order to deal with this, additional seeds are set in the model before calling the tree growing algorithm.

## K - Nearest Neighbour
Thomas

## Random Forest

For the RandomForest method, the package RandomForest was initially used. As usual, model performance will be evaluated using several folds of the data. The RandomForest package (and identically named method) contain several tuning parameters. In the loop below we will focus on manually tuning four parameters, namely the number of trees to grow, "mtry" or the number of variables to sample at each split, the sample fraction and finally, the minimum node size at the final branch of each tree. 

From initial investigations it was determined that models performed best under a form of boosting in which re-sampling of data was allowed, this corresponds to the "replace = "true" argument.


First we set a baseline with an un-tuned tree. The number of trees is varied from the baseline 500 to a relatively large (but still computationally tractable number of 3750)
```{r}
RF_tree_var <- matrix(NA, nrow=15, ncol=3)
colnames(RF_tree_var) <- c("data_split_seed","MSE - low tree","MSE - high tree")
i = 1

Tree_seed = 1996

for (data_seed_value in Data_splitting_seed_set){
  #create 80/20 split based on the data split seed. (outer loop)
  set.seed(data_seed_value)
  train_ind <- sample(1:nrow(DT_fact), 0.8*nrow(DT_fact))
  
  train <- DT_fact[train_ind,]
  test <- DT_fact[-train_ind,]
  
  Y_test <- test$wage_eur
  
  #teach actual model
  set.seed(Tree_seed) #fix for stocastic methods
  RF_low <- randomForest(wage_eur ~ ., data = train[, -"player_ID"], importance = TRUE, ntree = 500) # this will take a while
  
  set.seed(Tree_seed) #fix for stocastic methods
  RF_high <- randomForest(wage_eur ~ ., data = train[, -"player_ID"], importance = TRUE, ntree = 3750) # this will take a while
  
  #after model is made, calculate the MSE
  Y_pred_RF_low <- predict(RF_low, test)
  MSE_RF_LOW <- mean((Y_test - Y_pred_RF_low)^2)
  
  Y_pred_RF_high <- predict(RF_high, test)
  MSE_RF_HIGH <- mean((Y_test - Y_pred_RF_high)^2)
  
  #save results
  RF_tree_var[i, "data_split_seed"] <-  data_seed_value
  RF_tree_var[i, "MSE - low tree"] <-  MSE_RF_LOW
  RF_tree_var[i, "MSE - high tree"] <-  MSE_RF_HIGH
  
  #next row for next result
  i = i+1
}
RF_tree_var

```

We see that for the most part the trees with a larger ntree parameter perform better than their counterparts with lower Ntree parameter. In additon we observe quite a lot 
Annecdotally we can also inspect one tree to see how the branches are split, for which the last tree was chosen.
```{r}
vip(RF_high)
```


After seeing that more trees seemed to be slight beneficial we investigated the mtry parameter. A two stage tuning was performed. this tuning allowed us to pick an optimal (post prevelent )

#define the Tree based model in a function
#tree tuning block 1
```{r}

best_mtry_save <- matrix(NA, nrow=250, ncol=5)
colnames(best_mtry_save) <- c("data_split_seed","model_tune_seed","trees","best_mtry","best_OOB_error")
i = 1


#5 outer loops,
#5 model samples
#1 tree samples
#=5*5*1 = 25 options

for (data_split_seed_value in Data_splitting_seed_set){
  for (model_tune_seed_value in Model_stochastics_seed_set){
    for (num_of_trees in seq(1000, 1000, by=125)){
      #create 80/20 split based on the data split seed. (outer loop)
      set.seed(data_split_seed_value)
      train_ind <- sample(1:nrow(DT_fact), 0.8*nrow(DT_fact))
      
      train <- DT_fact[train_ind,]
      test <- DT_fact[-train_ind,]
      
      Y_test <- test$wage_eur
      
      #tune model (and since this is a stochastic process set seed for this)
      set.seed(model_tune_seed_value)
      
      #tune actual model
      mtry <- tuneRF(train[, -c("player_ID", "wage_eur")],train$wage_eur, ntreeTry=num_of_trees,
                     stepFactor=1.25,improve=0.01, trace=FALSE, plot=FALSE)
      best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
      best_m_oob <- mtry[mtry[, 2] == min(mtry[, 2]), 2]
      # print(mtry)
      # print(best.m)
      
      #save results from tuning
      best_mtry_save[i, "data_split_seed"] <-  data_split_seed_value
      best_mtry_save[i, "model_tune_seed"] <-  model_tune_seed_value
      best_mtry_save[i, "best_mtry"] <-  best.m
      best_mtry_save[i, "trees"] <-  num_of_trees
      best_mtry_save[i, "best_OOB_error"] <-  best_m_oob
      
      
      
      #next row for next result
      i = i+1
    }
  }
}


  
```

Now we evaluate and choose the best mtry

```{r}
min(best_mtry_save)
```

#tree tuning block 2
```{r}

#now run forest based on the optimum found in the previous set and get a baseline MSE over several seedings
mtry_chosen = 36

RF_outcome <- matrix(NA, nrow=500, ncol=5)
colnames(RF_outcome) <- c("data_split_seed","model_seed_value","trees","mtry","MSE")
i = 1

#5 outer loops,
#5 model samples
#1 tree samples
#=5*5*1 = 25 options

for (data_split_seed_value in Data_splitting_seed_set){
  for (model_tune_seed_value in Model_stochastics_seed_set){
    for (num_of_trees in seq(1000, 1000, by=200)){
      #create 80/20 split based on the data split seed. (outer loop)
      set.seed(model_seed_value)
      train_ind <- sample(1:nrow(DT_fact), 0.8*nrow(DT_fact))
      
      train <- DT_fact[train_ind,]
      test <- DT_fact[-train_ind,]
      
      Y_test <- test$wage_eur
      
      #tune model (and since this is a stochastic process set seed for this)
      set.seed(model_seed_value)
      
      #teach actual model
      RF <- randomForest(wage_eur ~ ., data = train[, -"player_ID"], importance = TRUE, ntree = num_of_trees) # this will take a while
      
      #after model is made, calculate the MSE
      loop_Y_pred_RF <- predict(RF, test[, -c("player_ID","wage_eur")])
      loop_MSE_RF = mean( (Y_test - loop_Y_pred_RF)^2 )
      
      #save results from tuning
      RF_outcome[i, "data_split_seed"] <-  data_split_seed_value
      RF_outcome[i, "model_seed_value"] <-  model_seed_value
      RF_outcome[i, "mtry"] <-  mtry_chosen
      RF_outcome[i, "trees"] <-  num_of_trees
      RF_outcome[i, "MSE"] <-  loop_MSE_RF
      
      #next row for next result
      i = i+1
    }
  }
}
```


We see that after tuning the Mtry parameter it was altered slightly from the previous baseline of sqrt(p). As is seen, the MSE from the evaluated sets is slightly lower than the baselines derived at the beginning of the section. Next we moved on to tuning the node size. For this the package "tuneRanger" was used. this package is based on a paper by [Probst et al, 2019](https://arxiv.org/abs/1804.03515)  and builds on MLR and the ranger tree packages. This package not only tunes the node parameter but also does the mtry and sample fraction parameters.

First we start by defining the task using the regr.task framework of MLR. For this example we use the full dataset as the package does internal cross validation. but smaller samples were used in similar loops as the examples shown above in order to evaluate the performance of the tuned model vs the other two RF trees.

```{r}
regr.task = makeRegrTask(id = "player_ID", data = DT_fact, target = "wage_eur")
```

Our evaluation method for the package was set as MSE. Others are also available, but this method stays in line with the previous method of testing.

```{r}
# Estimate runtime
estimateTimeTuneRanger(regr.task)

#set a fixed seed. From testing this one performed the best.
set.seed(123)

# Tuning
res = tuneRanger(regr.task, measure = list(mse), num.trees = 1000, iters = 70, iters.warmup = 30)
res

# Ranger Model with the new tuned hyperparameters
res$model
```

On average our MSE for the tuned model was around 10% lower than that of the untuned tree based models, which were already better performing that most other regression methods. The large drawback lies in the computational times for the tree based model. this fact is only increased by the intervention of the addiotional tuning step. Although longer, the "toughest iterations" were still able to be completed in around 20 to 25 minutes which makes the model just about tractable enough for the application we are now interested in.

```{r}
# our_prediction <- predict(RF, test_submission)
our_prediction <- predict(res$model, newdata = test_submission_fact)
View(our_prediction$data)
```

```{r}
#sample_submission <- fread("https://www.dropbox.com/s/cap0jhc8uxv5u26/sampleSubmission.csv?dl=1")

submit <- data.table(Id = test_submission_fact$player_ID, Predicted = our_prediction$data)
fwrite(submit, "./2020_02_27_submit_tree_tuned_by_tuneRanger.csv")


```



## Boosted Tree (XGBOOST?)
Huong


# support vector machine 
Fidel

# neural networks & ensemble methods


# Support Vector Machines
Robin