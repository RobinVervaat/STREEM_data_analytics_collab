---
title: "Data analytics for economists: Assignment I"
author: "Robin"
date: "2/27/2021"
output: 
  html_document:
    number_sections: TRUE
---

<style type="text/css">
body{ font-size: 15pt;}
pre {font-size: 12px;}
p {line-height: 1.5em;}
p {margin-bottom: 1em;}
p {margin-top: 1em;}
</style>

```{r include=FALSE}
knitr::opts_chunk$set(cache=F) # change it to T if you want to store results already produced. Note that the saved results will not be overwritten if you did not name each code chunk. Use it with caution.
```



## Random Forest


Forest based, or in more general terms tree based methods involve stratifying or segmenting the predictor space into a number of simple regions. To make a prediction for a given observation, we can use the mean or the mode of the training observations in the region to which it belongs. This method is quite intuitive in its working and can be graphically depicted. Below we see an example of how the paramater space can be divided using two independent variables.

| ![Hermon Trees](https://drive.google.com/uc?export=view&id=1RrtaJEHcNfXOqTSAIwnFOs3RgY-KTZX1) | 
|:--:| 
| *Space* Recursive binary splitting example with two predictors. Source: Hyeokmoon Kweon, Data analytics for economists, VU Amsterdam |


For the RandomForest method, the package RandomForest was initially used. As usual, model performance will be evaluated using several folds of the data. The RandomForest package (and identically named method) contain several tuning parameters. In the loop below we will focus on manually tuning four parameters, namely the number of trees to grow, "mtry" or the number of variables to sample at each split, the sample fraction and finally, the minimum node size at the final branch of each tree. 

From initial investigations it was determined that models performed best under a form of boosting in which re-sampling of data was allowed, this corresponds to the "replace = "true" argument.


First we set a baseline with an un-tuned tree. The number of trees is varied from the baseline 500 to a relatively large (but still computationally tractable number of 3750)
```{r eval=FALSE}
RF_tree_var <- matrix(NA, nrow=15, ncol=3)
colnames(RF_tree_var) <- c("data_split_seed","MSE - low tree","MSE - high tree")
i = 1

Tree_seed = 1996

for (data_seed_value in Data_splitting_seed_set){
  #create 80/20 split based on the data split seed. (outer loop)
  set.seed(data_seed_value)
  train_ind <- sample(1:nrow(DT_fact), 0.8*nrow(DT_fact))
  
  train <- DT_fact[train_ind,]
  test <- DT_fact[-train_ind,]
  
  Y_test <- test$wage_eur
  
  #teach actual model
  set.seed(Tree_seed) #fix for stocastic methods
  RF_low <- randomForest(wage_eur ~ ., data = train[, -"player_ID"], importance = TRUE, ntree = 500) # this will take a while
  
  set.seed(Tree_seed) #fix for stocastic methods
  RF_high <- randomForest(wage_eur ~ ., data = train[, -"player_ID"], importance = TRUE, ntree = 3750) # this will take a while
  
  #after model is made, calculate the MSE
  Y_pred_RF_low <- predict(RF_low, test)
  MSE_RF_LOW <- mean((Y_test - Y_pred_RF_low)^2)
  
  Y_pred_RF_high <- predict(RF_high, test)
  MSE_RF_HIGH <- mean((Y_test - Y_pred_RF_high)^2)
  
  #save results
  RF_tree_var[i, "data_split_seed"] <-  data_seed_value
  RF_tree_var[i, "MSE - low tree"] <-  MSE_RF_LOW
  RF_tree_var[i, "MSE - high tree"] <-  MSE_RF_HIGH
  
  #next row for next result
  i = i+1
}
RF_tree_var

```

We see that for the most part the trees with a larger ntree parameter perform better than their counterparts with lower Ntree parameter. In additon we observe quite a lot 
Annecdotally we can also inspect one tree to see how the branches are split, for which the last tree was chosen.
```{r eval=FALSE}
vip(RF_high)
```


After seeing that more trees seemed to be slight beneficial we investigated the mtry parameter. A two stage tuning was performed. this tuning allowed us to pick an optimal (post prevelent )

define the Tree based model in a function: tree tuning block 1
```{r eval=FALSE}

best_mtry_save <- matrix(NA, nrow=250, ncol=5)
colnames(best_mtry_save) <- c("data_split_seed","model_tune_seed","trees","best_mtry","best_OOB_error")
i = 1


#5 outer loops,
#5 model samples
#1 tree samples
#=5*5*1 = 25 options

for (data_split_seed_value in Data_splitting_seed_set){
  for (model_tune_seed_value in Model_stochastics_seed_set){
    for (num_of_trees in seq(1000, 1000, by=125)){
      #create 80/20 split based on the data split seed. (outer loop)
      set.seed(data_split_seed_value)
      train_ind <- sample(1:nrow(DT_fact), 0.8*nrow(DT_fact))
      
      train <- DT_fact[train_ind,]
      test <- DT_fact[-train_ind,]
      
      Y_test <- test$wage_eur
      
      #tune model (and since this is a stochastic process set seed for this)
      set.seed(model_tune_seed_value)
      
      #tune actual model
      mtry <- tuneRF(train[, -c("player_ID", "wage_eur")],train$wage_eur, ntreeTry=num_of_trees,
                     stepFactor=1.25,improve=0.01, trace=FALSE, plot=FALSE)
      best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
      best_m_oob <- mtry[mtry[, 2] == min(mtry[, 2]), 2]
      # print(mtry)
      # print(best.m)
      
      #save results from tuning
      best_mtry_save[i, "data_split_seed"] <-  data_split_seed_value
      best_mtry_save[i, "model_tune_seed"] <-  model_tune_seed_value
      best_mtry_save[i, "best_mtry"] <-  best.m
      best_mtry_save[i, "trees"] <-  num_of_trees
      best_mtry_save[i, "best_OOB_error"] <-  best_m_oob
      
      
      
      #next row for next result
      i = i+1
    }
  }
}


  
```

Now we evaluate and choose the best mtry

```{r eval=FALSE}
min(best_mtry_save)
```

tree tuning block 2
```{r eval=FALSE}

#now run forest based on the optimum found in the previous set and get a baseline MSE over several seedings
mtry_chosen = 36

RF_outcome <- matrix(NA, nrow=500, ncol=5)
colnames(RF_outcome) <- c("data_split_seed","model_seed_value","trees","mtry","MSE")
i = 1

#5 outer loops,
#5 model samples
#1 tree samples
#=5*5*1 = 25 options

for (data_split_seed_value in Data_splitting_seed_set){
  for (model_tune_seed_value in Model_stochastics_seed_set){
    for (num_of_trees in seq(1000, 1000, by=200)){
      #create 80/20 split based on the data split seed. (outer loop)
      set.seed(model_seed_value)
      train_ind <- sample(1:nrow(DT_fact), 0.8*nrow(DT_fact))
      
      train <- DT_fact[train_ind,]
      test <- DT_fact[-train_ind,]
      
      Y_test <- test$wage_eur
      
      #tune model (and since this is a stochastic process set seed for this)
      set.seed(model_seed_value)
      
      #teach actual model
      RF <- randomForest(wage_eur ~ ., data = train[, -"player_ID"], importance = TRUE, ntree = num_of_trees) # this will take a while
      
      #after model is made, calculate the MSE
      loop_Y_pred_RF <- predict(RF, test[, -c("player_ID","wage_eur")])
      loop_MSE_RF = mean( (Y_test - loop_Y_pred_RF)^2 )
      
      #save results from tuning
      RF_outcome[i, "data_split_seed"] <-  data_split_seed_value
      RF_outcome[i, "model_seed_value"] <-  model_seed_value
      RF_outcome[i, "mtry"] <-  mtry_chosen
      RF_outcome[i, "trees"] <-  num_of_trees
      RF_outcome[i, "MSE"] <-  loop_MSE_RF
      
      #next row for next result
      i = i+1
    }
  }
}
```


We see that after tuning the Mtry parameter it was altered slightly from the previous baseline of sqrt(p). As is seen, the MSE from the evaluated sets is slightly lower than the baselines derived at the beginning of the section. Next we moved on to tuning the node size. For this the package "tuneRanger" was used. this package is based on a paper by [Probst et al, 2019](https://arxiv.org/abs/1804.03515)  and builds on MLR and the ranger tree packages. This package not only tunes the node parameter but also does the mtry and sample fraction parameters.

First we start by defining the task using the regr.task framework of MLR. For this example we use the full dataset as the package does internal cross validation. but smaller samples were used in similar loops as the examples shown above in order to evaluate the performance of the tuned model vs the other two RF trees.

```{r eval=FALSE}
regr.task = makeRegrTask(id = "player_ID", data = DT_fact, target = "wage_eur")
```

Our evaluation method for the package was set as MSE. Others are also available, but this method stays in line with the previous method of testing.

```{r eval=FALSE}
# Estimate runtime
estimateTimeTuneRanger(regr.task)

#set a fixed seed. From testing this one performed the best.
set.seed(123)

# Tuning
res = tuneRanger(regr.task, measure = list(mse), num.trees = 1000, iters = 70, iters.warmup = 30)
res

# Ranger Model with the new tuned hyperparameters
res$model
```

On average our MSE for the tuned model was around 10% lower than that of the untuned tree based models, which were already better performing that most other regression methods. The large drawback lies in the computational times for the tree based model. this fact is only increased by the intervention of the addiotional tuning step. Although longer, the "toughest iterations" were still able to be completed in around 20 to 25 minutes which makes the model just about tractable enough for the application we are now interested in.

```{r eval=FALSE}
# our_prediction <- predict(RF, test_submission)
our_prediction <- predict(res$model, newdata = test_submission_fact)
View(our_prediction$data)
```

```{r eval=FALSE}
#sample_submission <- fread("https://www.dropbox.com/s/cap0jhc8uxv5u26/sampleSubmission.csv?dl=1")

submit <- data.table(Id = test_submission_fact$player_ID, Predicted = our_prediction$data)
fwrite(submit, "./2020_02_27_submit_tree_tuned_by_tuneRanger.csv")

```

We see that of the methods evaluated so far, the random forest seems quite robust for the presence of outliers, something that seemed to hurt the performance of the earlier linear based prediction methods. The drawback however is that what we gain in robustness (and for that case performance) we loose in understanding of the model. THe randomforest model essentially operates as a black box with little to no information on the exact operation. We can regain some of this information by examing predictor importance plots, but they still remain far less interpretable than the earlier models discussed.

Besides this we did see great improvements when the model is tuned. This tuning took some computational effort and most of all memory resources. This fact alone made the randomforest model one of the most intensive methods evaluated. In conclusion we are quite happy with the perdormance of the randomforest model. It takes some effort to tune the model, but the overall performance and ease of use is definitely worth the investment.
