---
title: "Data analytics for economists: Assignment I"
author: "Robin"
date: "2/27/2021"
output: 
  html_document:
    number_sections: TRUE
---

<style type="text/css">
body{ font-size: 15pt;}
pre {font-size: 12px;}
p {line-height: 1.5em;}
p {margin-bottom: 1em;}
p {margin-top: 1em;}
</style>

```{r include=FALSE}
knitr::opts_chunk$set(cache=F) # change it to T if you want to store results already produced. Note that the saved results will not be overwritten if you did not name each code chunk. Use it with caution.
```




***


# Document overhead and initialisation

##Read in packages

```{r}
library(data.table)
library(ggplot2)


library(tuneRanger)
library(mlr)
library(OpenML)

library(e1071)

pacman::p_load(rpart, rpart.plot, vip, pROC, randomForest, gbm)


```

## Download the data and present some basic statistics.
1 Download test and training sets. 
-- Training = DT
-- Test = test_submission

2 create copies of the data to enable processing of different data sets for different methods

```{r}
DT_raw <- fread("https://www.dropbox.com/s/5rr2ysw6tjcnbpj/train.csv?dl=1")
summary(DT_raw)

test_submission_raw <- fread("https://www.dropbox.com/s/395thqjfxf7k4wi/test.csv?dl=1")

DT_fact <- copy(DT_raw)
test_submission_fact <- copy(test_submission_raw)


```

Something we do note from the distribution of wage shows some outliers. We hypothesise this is the top players such as the Ronaldo's and Messi's of the world. We however also must not that there is likely no (linear) relationship to only the skill of the player. Likely these players their salary is in part so high since they are very popular with fans and bring in additional revenue through that popularity (e.g. sponsorship deals.).

The histogram below depicts the effect very well that was hinted at by the summary statistics

```{r}
hist(DT_raw$wage_eur,  breaks = 1500, xlab = "Wage in Euros")
```


## Some data processing is needed to meld the data into certain applications 
Factorize data. Manual process to ensure consistency for both the test and training sets

```{r}
#DT dataset
DT_fact$preferred_foot <- invisible(sapply(DT_fact$preferred_foot,switch,'Right'=1,'Left'=2,'undefined'=""))
DT_fact$work_rate <- invisible(sapply(DT_fact$work_rate,switch,'Low/Medium'=1,'High/Medium'=2,'Medium/Medium'=3,'Medium/Low'=4,'Medium/High'=5,'High/High'=6,'High/Low'=7,'Low/Low'=8,'Low/High'=9,'undefined'=""))
DT_fact$team_position <- invisible(sapply(DT_fact$team_position,switch,'LCB'=1,'SUB'=2,'RES'=3,'LS'=4,'RB'=5,'ST'=6,'RM'=7,'LCM'=8,'LB'=9,'LM'=10,'RCB'=11,'RW'=12,'CAM'=13,'RDM'=14,'RWB'=15,'LF'=16,'CDM'=17,'LW'=18,'CB'=19,'LDM'=20,'CM'=21,'RCM'=22,'LAM'=23,'CF'=24,'RS'=25,'RF'=26,'RAM'=27,'LWB'=28,'undefined'=""))

#Test submission factorization. Identical factorisation criteria
test_submission_fact$preferred_foot <- invisible(sapply(test_submission_fact$preferred_foot,switch,'Right'=1,'Left'=2,'undefined'=""))
test_submission_fact$work_rate <- invisible(sapply(test_submission_fact$work_rate,switch,'Low/Medium'=1,'High/Medium'=2,'Medium/Medium'=3,'Medium/Low'=4,'Medium/High'=5,'High/High'=6,'High/Low'=7,'Low/Low'=8,'Low/High'=9,'undefined'=""))
test_submission_fact$team_position <- invisible(sapply(test_submission_fact$team_position,switch,'LCB'=1,'SUB'=2,'RES'=3,'LS'=4,'RB'=5,'ST'=6,'RM'=7,'LCM'=8,'LB'=9,'LM'=10,'RCB'=11,'RW'=12,'CAM'=13,'RDM'=14,'RWB'=15,'LF'=16,'CDM'=17,'LW'=18,'CB'=19,'LDM'=20,'CM'=21,'RCM'=22,'LAM'=23,'CF'=24,'RS'=25,'RF'=26,'RAM'=27,'LWB'=28,'undefined'=""))

```


### Important parameters defining the regression model
```{r}
Data_splitting_seed_set = seq(120,124, by = 1) #seeds through which ten folds of the data are created for evaluation of methods
Model_stochastics_seed_set = seq(200,204, by = 1) #set of seeds to initiate stochastic models. aids in reproducibility

```


# Regression methods

In the section below we discuss and evaluate several machine leaning methods to predict footballers wage as a function of a broad set of performance criteria. The section and methods can be broadly broken up into 4 main methods:

1 regression-based methods
2 tree-based methods
3 support vector machine 
4 neural networks & ensemble methods

Each section will enjoy discussion and will feature several sub-methods. The method for comparison is mainly focused around the Mean Squared Error or MSE for short. This metric is easy to compute and facilitates easy comparison and aggreation between methods.Of course, as the sections will show, other metrics are considered in the tuning and selection of methods. Finally, the best performing model will be highlighted in a seperate subsection alongside which a discussion of the great performance of this method will be given.


## regression-based methods

### OLS
We start with the most basic method evaluated in the course of this document Ordinary Least Squared or OLS for short. OLS determines the best linear combination of estimators to determine the independent variable. In order to get a robust statistic for OLS (and as we will repeat for every method) we generate several folds of data according to the  seed parameters defined earlier in the document.



```{r}

OLS_outcome <- matrix(NA, nrow=10, ncol=2)
colnames(OLS_outcome) <- c("data_split_seed","MSE")
i = 1

for (data_seed_value in Data_splitting_seed_set){
  #create 80/20 split based on the data split seed. (outer loop)
  set.seed(data_seed_value)
  train_ind <- sample(1:nrow(DT_fact), 0.8*nrow(DT_fact))
  
  train <- DT_fact[train_ind,]
  test <- DT_fact[-train_ind,]
  
  Y_test <- test$wage_eur
  
  #teach actual model
  OLS_mod <- lm(wage_eur ~ ., train[, -c("player_ID")])
  
  #after model is made, calculate the MSE
  Y_pred_ols <- predict(OLS_mod, test)
  MSE <- mean((Y_test - Y_pred_ols)^2)
  
  #save results
  OLS_outcome[i, "data_split_seed"] <-  data_seed_value
  OLS_outcome[i, "MSE"] <-  MSE
  
  #next row for next result
  i = i+1
}

```


### A more advanced method considered was the elasticnet. Elasticnet is a combination of Ridge and Lasso regression.

HUONG will do this section!





# Tree based models
Tree based models are another popular modeling method in Machine learning. ALthough often though of as a classification method, Trees can also be used to make numerical predictions. In the section below a handful of tree based methods are evaluated. Overall the tree based metrics are some of the best performing of all evaluated methods. 

Tree based methods are oftentimes stochastic in nature. In order to deal with this, additional seeds are set in the model before calling the tree growing algorithm.

## K - Nearest Neighbour
Thomas

## Random Forest

For the RandomForest method, the package RandomForest was initially used. As usual, model performance will be evaluated using several folds of the data. The RandomForest package (and identically named method) contain several tuning parameters. In the loop below we will focus on manually tuning four parameters, namely the number of trees to grow, "mtry" or the number of variables to sample at each split, the sample fraction and finally, the minimum node size at the final branch of each tree. 

From initial investigations it was determined that models performed best under a form of boosting in which re-sampling of data was allowed, this corresponds to the "replace = "true" argument.


First we set a baseline with an un-tuned tree. The number of trees is varied from the baseline 500 to a relatively large (but still computationally tractable number of 3750)
```{r}
RF_tree_var <- matrix(NA, nrow=15, ncol=3)
colnames(RF_tree_var) <- c("data_split_seed","MSE - low tree","MSE - high tree")
i = 1

Tree_seed = 1996

for (data_seed_value in Data_splitting_seed_set){
  #create 80/20 split based on the data split seed. (outer loop)
  set.seed(data_seed_value)
  train_ind <- sample(1:nrow(DT_fact), 0.8*nrow(DT_fact))
  
  train <- DT_fact[train_ind,]
  test <- DT_fact[-train_ind,]
  
  Y_test <- test$wage_eur
  
  #teach actual model
  set.seed(Tree_seed) #fix for stocastic methods
  RF_low <- randomForest(wage_eur ~ ., data = train[, -"player_ID"], importance = TRUE, ntree = 500) # this will take a while
  
  set.seed(Tree_seed) #fix for stocastic methods
  RF_high <- randomForest(wage_eur ~ ., data = train[, -"player_ID"], importance = TRUE, ntree = 3750) # this will take a while
  
  #after model is made, calculate the MSE
  Y_pred_RF_low <- predict(RF_low, test)
  MSE_RF_LOW <- mean((Y_test - Y_pred_RF_low)^2)
  
  Y_pred_RF_high <- predict(RF_high, test)
  MSE_RF_HIGH <- mean((Y_test - Y_pred_RF_high)^2)
  
  #save results
  RF_tree_var[i, "data_split_seed"] <-  data_seed_value
  RF_tree_var[i, "MSE - low tree"] <-  MSE_RF_LOW
  RF_tree_var[i, "MSE - high tree"] <-  MSE_RF_HIGH
  
  #next row for next result
  i = i+1
}
RF_tree_var

```

We see that for the most part the trees with a larger ntree parameter perform better than their counterparts with lower Ntree parameter. In additon we observe quite a lot 
Annecdotally we can also inspect one tree to see how the branches are split, for which the last tree was chosen.
```{r}
vip(RF_high)
```


After seeing that more trees seemed to be slight beneficial we investigated the mtry parameter. A two stage tuning was performed. this tuning allowed us to pick an optimal (post prevelent )

#define the Tree based model in a function
#tree tuning block 1
```{r}

best_mtry_save <- matrix(NA, nrow=250, ncol=5)
colnames(best_mtry_save) <- c("data_split_seed","model_tune_seed","trees","best_mtry","best_OOB_error")
i = 1


#5 outer loops,
#5 model samples
#1 tree samples
#=5*5*1 = 25 options

for (data_split_seed_value in Data_splitting_seed_set){
  for (model_tune_seed_value in Model_stochastics_seed_set){
    for (num_of_trees in seq(1000, 1000, by=125)){
      #create 80/20 split based on the data split seed. (outer loop)
      set.seed(data_split_seed_value)
      train_ind <- sample(1:nrow(DT_fact), 0.8*nrow(DT_fact))
      
      train <- DT_fact[train_ind,]
      test <- DT_fact[-train_ind,]
      
      Y_test <- test$wage_eur
      
      #tune model (and since this is a stochastic process set seed for this)
      set.seed(model_tune_seed_value)
      
      #tune actual model
      mtry <- tuneRF(train[, -c("player_ID", "wage_eur")],train$wage_eur, ntreeTry=num_of_trees,
                     stepFactor=1.25,improve=0.01, trace=FALSE, plot=FALSE)
      best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
      best_m_oob <- mtry[mtry[, 2] == min(mtry[, 2]), 2]
      # print(mtry)
      # print(best.m)
      
      #save results from tuning
      best_mtry_save[i, "data_split_seed"] <-  data_split_seed_value
      best_mtry_save[i, "model_tune_seed"] <-  model_tune_seed_value
      best_mtry_save[i, "best_mtry"] <-  best.m
      best_mtry_save[i, "trees"] <-  num_of_trees
      best_mtry_save[i, "best_OOB_error"] <-  best_m_oob
      
      
      
      #next row for next result
      i = i+1
    }
  }
}


  
```

Now we evaluate and choose the best mtry

```{r}
min(best_mtry_save)
```

#tree tuning block 2
```{r}

#now run forest based on the optimum found in the previous set and get a baseline MSE over several seedings
mtry_chosen = 36

RF_outcome <- matrix(NA, nrow=500, ncol=5)
colnames(RF_outcome) <- c("data_split_seed","model_seed_value","trees","mtry","MSE")
i = 1

#5 outer loops,
#5 model samples
#1 tree samples
#=5*5*1 = 25 options

for (data_split_seed_value in Data_splitting_seed_set){
  for (model_tune_seed_value in Model_stochastics_seed_set){
    for (num_of_trees in seq(1000, 1000, by=200)){
      #create 80/20 split based on the data split seed. (outer loop)
      set.seed(model_seed_value)
      train_ind <- sample(1:nrow(DT_fact), 0.8*nrow(DT_fact))
      
      train <- DT_fact[train_ind,]
      test <- DT_fact[-train_ind,]
      
      Y_test <- test$wage_eur
      
      #tune model (and since this is a stochastic process set seed for this)
      set.seed(model_seed_value)
      
      #teach actual model
      RF <- randomForest(wage_eur ~ ., data = train[, -"player_ID"], importance = TRUE, ntree = num_of_trees) # this will take a while
      
      #after model is made, calculate the MSE
      loop_Y_pred_RF <- predict(RF, test[, -c("player_ID","wage_eur")])
      loop_MSE_RF = mean( (Y_test - loop_Y_pred_RF)^2 )
      
      #save results from tuning
      RF_outcome[i, "data_split_seed"] <-  data_split_seed_value
      RF_outcome[i, "model_seed_value"] <-  model_seed_value
      RF_outcome[i, "mtry"] <-  mtry_chosen
      RF_outcome[i, "trees"] <-  num_of_trees
      RF_outcome[i, "MSE"] <-  loop_MSE_RF
      
      #next row for next result
      i = i+1
    }
  }
}
```


We see that after tuning the Mtry parameter it was altered slightly from the previous baseline of sqrt(p). As is seen, the MSE from the evaluated sets is slightly lower than the baselines derived at the beginning of the section. Next we moved on to tuning the node size. For this the package "tuneRanger" was used. this package is based on a paper by [Probst et al, 2019](https://arxiv.org/abs/1804.03515)  and builds on MLR and the ranger tree packages. This package not only tunes the node parameter but also does the mtry and sample fraction parameters.

First we start by defining the task using the regr.task framework of MLR. For this example we use the full dataset as the package does internal cross validation. but smaller samples were used in similar loops as the examples shown above in order to evaluate the performance of the tuned model vs the other two RF trees.

```{r}
regr.task = makeRegrTask(id = "player_ID", data = DT_fact, target = "wage_eur")
```

Our evaluation method for the package was set as MSE. Others are also available, but this method stays in line with the previous method of testing.

```{r}
# Estimate runtime
estimateTimeTuneRanger(regr.task)

#set a fixed seed. From testing this one performed the best.
set.seed(123)

# Tuning
res = tuneRanger(regr.task, measure = list(mse), num.trees = 1000, iters = 70, iters.warmup = 30)
res

# Ranger Model with the new tuned hyperparameters
res$model
```

On average our MSE for the tuned model was around 10% lower than that of the untuned tree based models, which were already better performing that most other regression methods. The large drawback lies in the computational times for the tree based model. this fact is only increased by the intervention of the addiotional tuning step. Although longer, the "toughest iterations" were still able to be completed in around 20 to 25 minutes which makes the model just about tractable enough for the application we are now interested in.

```{r}
# our_prediction <- predict(RF, test_submission)
our_prediction <- predict(res$model, newdata = test_submission_fact)
View(our_prediction$data)
```

```{r}
#sample_submission <- fread("https://www.dropbox.com/s/cap0jhc8uxv5u26/sampleSubmission.csv?dl=1")

submit <- data.table(Id = test_submission_fact$player_ID, Predicted = our_prediction$data)
fwrite(submit, "./2020_02_27_submit_tree_tuned_by_tuneRanger.csv")


```



## Boosted Tree (XGBOOST?)
Huong


# support vector machine 
Fidel

# neural networks & ensemble methods


# Support Vector Machines
Support vector machine learning is a method in which the data is iteratively broken up through hyperplanes in n-dimensional space. SVM is best understood by examining graphically what it does in a simplified 2 dimensional space. The image below gives an illustrations of such hyperplane dividing data in two spaces (and eventually) such that the divide is as strong as possible. This latter point is measured by the margin (function) and is the reason SVM is also known as a maximal margin classifier.

As one can imagine in these cases the data closest to the plane will weigh much stronger on the shape of the hyperplane than observations far away. 

Inline-style: 
![alt text](https://techvidvan.com/tutorials/wp-content/uploads/sites/2/2020/05/Support-Vector-Machines-1.jpg)

The basis for our SVM analysis is the package E1071.This package is quite user friendly and allowed us to tune the model through several parameters. However, to set a baseline a basic SVM regression on a simple 80/20 split fo the data was performed to evaluate computational efforts and evaluate tuning strategies.

```{r}

#create data split with reproducibility 
set.seed(1996)
train_ind <- sample(1:nrow(DT_fact), 0.8*nrow(DT_fact))

train <- DT_fact[train_ind,]
test <- DT_fact[-train_ind,]

Y_test <- test$wage_eur

#since the model will contain some stochastics, it is important to set a seed.
set.seed(1996)

#Train model on training set. Making sure to exclude player ID
svm_model <- svm(wage_eur ~ ., data = train[, -"player_ID"])

#show some basic parameters defining the SVM model
summary(svm_model)


#after model is made, calculate the MSE
SVM_predictions <- predict(svm_model, test[, -c("player_ID","wage_eur")])
SVM_MSE = mean( (Y_test - SVM_predictions)^2 )
```
The model trains in about one minute, not too bad. The results however, as compared to the tree model are quite similar. The gap with the baseline (untuned) Random forest model is 14 million / 306 million = around 5%. Perhaps tuning can help the model be more competitive. The computational effort in any case seems quite a lot lower.
```{r}
SVM_MSE
```
Before continuing a quick investigation into the application of shrinking in terms of the SVM function was evaluated. Shrinking works in a similar way to the penalization method which we have discussed in earlier parts of this paper. By default the SVM from the E1071 package has shrinkage on, but from ealier investigations it was shown that pure shrinkage might not always yield the best results. In order to investigate we put this assumption to the test! We take 5 seeds and evaluate the effect of having this active vs. not active, below we present the results.

```{r}
shrinkage_test <- matrix(NA, nrow=250, ncol=4)
colnames(shrinkage_test) <- c("data_split_seed","model_tune_seed","shrinkage","MSE")
i = 1

for (data_split_seed_value in 1:5){
  for (shrink_tf in c(TRUE,FALSE)){
    #create data split with reproducibility 
    set.seed(data_split_seed_value)
    train_ind <- sample(1:nrow(DT_fact), 0.8*nrow(DT_fact))
    
    train <- DT_fact[train_ind,]
    test <- DT_fact[-train_ind,]
    
    Y_test <- test$wage_eur
    
    #since the model will contain some stochastic, it is important to set a seed.
    set.seed(1996)
    
    #Train model on training set. Making sure to exclude player ID
    svm_model <- svm(wage_eur ~ ., data = train[, -"player_ID"], shrinking = shrink_tf)
    

    #after model is made, calculate the MSE
    SVM_predictions <- predict(svm_model, test[, -c("player_ID","wage_eur")])
    SVM_MSE = mean( (Y_test - SVM_predictions)^2 )
    
    #save results from tuning
    shrinkage_test[i, "data_split_seed"] <-  data_split_seed_value
    shrinkage_test[i, "model_tune_seed"] <-  1996
    shrinkage_test[i, "shrinkage"] <-  shrink_tf
    shrinkage_test[i, "MSE"] <-  SVM_MSE
    
    
    
    #next row for next result
    i = i+1
  }
}



```

```{r}
shrinkage_test
```
Looking at the data we see that with shrinkage on, the estimations are every so slightly better in every case and as such we continue with this active for future predictions.

We now choose two more model parameters to further investigate. These model parameters are evaluated for the set of 5 seeds and then compared with one another.The first parameter investigated is cost parameter for the violation of regularization terms, at default this is one, but two higher and one lower value of 0.5,2 and 10 respectively were considered. Finally, we also took a look at the epsilon parameter. This value is by default 0.1, but will be varied from 0.05 to 0.2.


```{r}
SVM_output <- matrix(NA, nrow=250, ncol=5)
colnames(SVM_output) <- c("data_split_seed","model_tune_seed","cost","epsilon","MSE")
i = 1


#5 outer loops,
#4 cost items
#3 epsilon values
#= 5*4*3 = 60 options

for (data_split_seed_value in 1:5){
  for (cost_item in c(0.5,1,2,10)){
    for (epsilon_item in c(0.05,0.1,0.2)){
      
      #create data split with reproducibility 
      set.seed(data_split_seed_value)
      train_ind <- sample(1:nrow(DT_fact), 0.8*nrow(DT_fact))
      
      train <- DT_fact[train_ind,]
      test <- DT_fact[-train_ind,]
      
      Y_test <- test$wage_eur
      
      #since the model will contain some stochastic, it is important to set a seed.
      set.seed(1996)
      
      #Train model on training set. Making sure to exclude player ID
      svm_model <- svm(wage_eur ~ ., data = train[, -"player_ID"], epsilon = epsilon_item, cost = cost_item)
      
  
      #after model is made, calculate the MSE
      SVM_predictions <- predict(svm_model, test[, -c("player_ID","wage_eur")])
      SVM_MSE = mean( (Y_test - SVM_predictions)^2 )
      
      #save results from tuning
      SVM_output[i, "data_split_seed"] <-  data_split_seed_value
      SVM_output[i, "model_tune_seed"] <-  1996
      SVM_output[i, "cost"] <-  cost_item
      SVM_output[i, "epsilon"] <-  epsilon_item
      SVM_output[i, "MSE"] <-  SVM_MSE
      
      
      
      #next row for next result
      i = i+1
    }
  }
}


  
```


```{r}
SVM_output
```

A quick analysis of the data shows that changing the epsilon has little effect. On average, the best performing lambda the stock lambda for all different values of the cost. The cost showed a far wider space of possible outcomes. The best performing cost paramater was also not the default value, but rather the highst cost paramater at a cost of 10. This is 10 times larger than the default value and managed to decrease the MSE by nearly 10% as compared to the baseline value of cost. Computational time between runs showed little to no difference.

Using the best evaluated values for parameters, a model training was performed with the full dataset in order to produce a prediction for the competition test data set.


```{r}
#best performing SVM model
svm_model_best <- svm(wage_eur ~ ., data = DT_fact[, -"player_ID"], epsilon = 0.1, cost = 10)

our_prediction <- predict(svm_model_best, test_submission)

submit <- data.table(Id = test_submission$player_ID, Predicted = our_prediction)
fwrite(submit, "./2020_02_28-best_performing_SVM.csv")
```



***
# Section name

## Subsection name

### Sub-subsection name


- bullet 1
- bullet 2

1. numbered item 1
2. numbered item 2






